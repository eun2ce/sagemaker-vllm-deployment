# Use DJL Python engine (vLLM + transformers)
engine=Python

# Use custom inference.py
option.entryPoint=inference

# Hugging Face model ID
option.model_id=your-model-id # e.g. Qwen/Qwen3-4B-Instruct-2507

# Maximum context length (input + output total)
# 32K: ~16GB VRAM (general chat)
# 128K: ~40GB VRAM (long documents)
# 256K: ~80GB VRAM (maximum)
option.max_model_len=32768
instance_type = ml.g5.xlarge

# For long document processing
# option.max_model_len=131072
# instance_type = ml.g5.12xlarge

# GPU memory utilization (0.0-1.0)
# 0.85: stable, 0.95: aggressive (OOM risk)
option.gpu_memory_utilization=0.85

# Multi-GPU distributed processing (1=single GPU)
option.tensor_parallel_size=1

# Task type: "text-generation" or "embedding"
option.task=text-generation  # or embedding
